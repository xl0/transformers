{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_tensors.patch import monkey_patch\n",
    "\n",
    "monkey_patch()\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the tiny stories dataset\n",
    "\n",
    "# TS_PATH = \"./ts/\"\n",
    "# TS_PATH = Path(TS_PATH)\n",
    "\n",
    "\n",
    "# gpt35_stories = []\n",
    "# gpt4_stories = []\n",
    "\n",
    "import unidecode\n",
    "\n",
    "# for file in tqdm(list(sorted(os.listdir(TS_PATH)))):\n",
    "#     if file.endswith(\".json\"):\n",
    "#         with open(TS_PATH / file, \"r\") as f:\n",
    "#             data = json.load(f)\n",
    "#             for d in data:\n",
    "#                 story = d[\"story\"]\n",
    "#                 if not all(ord(c) < 128 for c in story):\n",
    "#                     story = unidecode.unidecode(story)\n",
    "\n",
    "#                 if d[\"source\"] == \"GPT-3.5\":\n",
    "#                     gpt35_stories.append(story)\n",
    "#                 elif d[\"source\"] == \"GPT-4\":\n",
    "#                     gpt4_stories.append(story)\n",
    "\n",
    "# with open(\"gpt35_stories.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(gpt35_stories))\n",
    "\n",
    "# with open(\"gpt4_stories.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(gpt4_stories))\n",
    "\n",
    "with open(\"gpt35_stories.txt\", \"r\") as f:\n",
    "    gpt35_stories = f.read().split(\"\\n\")\n",
    "\n",
    "with open(\"gpt4_stories.txt\", \"r\") as f:\n",
    "    gpt4_stories = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\\n\".join(gpt35_stories + gpt4_stories)\n",
    "del gpt35_stories\n",
    "del gpt4_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "\n",
    "\n",
    "def decode(l):\n",
    "    return \"\".join(\n",
    "        [itos[i] for i in l]\n",
    "    )  # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\t\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz{|}~',\n",
       " 96)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(chars), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_data = encode(data)\n",
    "train_data = data[: int(len(data)) - 200_000]\n",
    "val_data = data[int(len(data)) - 200_000 :]\n",
    "del data\n",
    "\n",
    "# train_data = torch.tensor(data)\n",
    "# val_data = torch.tensor(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(data, ctx):\n",
    "    # i = random.randint(0, len(data) - ctx - 1)\n",
    "    i = 0\n",
    "    while i + ctx < len(data):\n",
    "        src = data[i : i + ctx]\n",
    "        dst = data[i + 1 : i + ctx + 1]\n",
    "        yield torch.tensor(encode(src)), torch.tensor(encode(dst))\n",
    "        i += ctx\n",
    "\n",
    "\n",
    "def get_epoch(data, ctx_len, batch_size, shuffle=True):\n",
    "    \"\"\"Yields a tuple of tensors of shape (batch_size, ctx).\n",
    "    X, shape=B C\n",
    "    y, shape=B C\n",
    "    \"\"\"\n",
    "\n",
    "    items = get_item(data, ctx_len)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            X, y = zip(*[next(items) for _ in range(batch_size)])\n",
    "            yield torch.stack(X), torch.stack(y)\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[2, 5] i64 n=10 x∈[2, 90] μ=61.400 σ=32.215 [[85, 70, 69, 69, 90], [2, 84, 73, 70, 2]],\n",
       " tensor[2, 5] i64 n=10 x∈[2, 90] μ=60.600 σ=31.659 [[70, 69, 69, 90, 2], [84, 73, 70, 2, 77]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_random_item(data, ctx):\n",
    "    i = random.randint(0, len(data) - ctx - 1)\n",
    "\n",
    "    src = data[i : i + ctx]\n",
    "    dst = data[i + 1 : i + ctx + 1]\n",
    "\n",
    "    return torch.tensor(encode(src)), torch.tensor(encode(dst))\n",
    "\n",
    "\n",
    "def get_batch(data, ctx_len, batch_size, shuffle=True):\n",
    "    \"\"\"Yields a tuple of tensors of shape (batch_size, ctx).\n",
    "    X, shape=B C\n",
    "    y, shape=B C\n",
    "    \"\"\"\n",
    "\n",
    "    batch = [get_random_item(data, ctx_len) for _ in range(batch_size)]\n",
    "    X, y = zip(*batch)\n",
    "\n",
    "    return torch.stack(X), torch.stack(y)\n",
    "\n",
    "\n",
    "get_batch(train_data[:100], ctx_len=5, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.2 ms ± 932 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(params, input_ids, vocab_size):\n",
    "    \"\"\"This model takes in a sequence and predicts 1 token\"\"\"\n",
    "\n",
    "    one_hot_inputs = torch.nn.functional.one_hot(input_ids, num_classes=vocab_size)\n",
    "    one_hot_inputs = one_hot_inputs.float()\n",
    "\n",
    "    embeddings = one_hot_inputs @ params[\"embedding\"].T  # N, CTX_LEN, EMBEDDING_DIM\n",
    "\n",
    "    # preds = hidden_states[:, -1, :] # @ params[\"w\"]\n",
    "\n",
    "    hidden_state = (\n",
    "        embeddings.view((input_ids.shape[0], -1)) @ params[\"w1\"].T + params[\"b1\"]\n",
    "    )\n",
    "    hidden_state = torch.nn.functional.relu(hidden_state)\n",
    "\n",
    "    hidden_state = (\n",
    "        hidden_state.view((input_ids.shape[0], -1)) @ params[\"w2\"].T + params[\"b2\"]\n",
    "    )\n",
    "    hidden_state = torch.nn.functional.relu(hidden_state)\n",
    "\n",
    "    preds = hidden_state @ params[\"embedding\"]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, model_params, ctx_len, encoded_prompt, n_tokens, temperature=0.0, top_k=1):\n",
    "    \"\"\"Generate n_tokens after prompt\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_tokens):\n",
    "            # print(encoded_prompt)\n",
    "            preds = model(\n",
    "                model_params, encoded_prompt[:, -ctx_len:], vocab_size=len(chars)\n",
    "            )\n",
    "\n",
    "            probs = torch.nn.functional.softmax(preds, dim=-1).pow(\n",
    "                1 / (temperature + 1e-6)\n",
    "            )\n",
    "\n",
    "            # Implementing top-k sampling\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "            next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "            next_token = top_k_indices.gather(dim=1, index=next_token)\n",
    "\n",
    "            encoded_prompt = torch.cat([encoded_prompt, next_token], dim=1)\n",
    "    return encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training data: 3,849,741,212\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the training data: {len(train_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xl0/work/projects/transformers/wandb/run-20230930_221556-1pyazj57</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llmnerds/my-awesome-project/runs/1pyazj57' target=\"_blank\">misty-galaxy-151</a></strong> to <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/1pyazj57' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/1pyazj57</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'d higher and'-> ' said, \"I want to play w'\n",
      "'the tree, go'-> ' he was and the bird fle'\n",
      "' wanted to e'-> 'at the bird and the bird'\n",
      "'s so happy h'-> 'er mom and dad. The bird'\n",
      "'very day. Li'-> 'ly was so happy and said'\n",
      "'s a huge bal'-> 'l and the big store and '\n",
      "'iny and had '-> 'a friends and said, \"You'\n",
      "'rry did not '-> 'like the story and said,'\n",
      "' who never g'-> 'ood and said, \"You are y'\n",
      "'ple told him'-> ' that he was so happy to'\n",
      "'mom was hurt'-> ' and said, \"Thank you, M'\n",
      "'d followed A'-> 'nna and saw a big tree w'\n",
      "' found a big'-> ' tree with a big surpris'\n",
      "'r dress?\"\\nSa'-> 'rah was so happy and the'\n",
      "'he tall buil'-> 'd and said, \"Thank you, '\n",
      "'rt. They cri'-> 'ed to play with her frie'\n",
      "'ould talk to'-> ' his friend and said, \"I'\n",
      "'ee. The bird'-> ' was so happy to have a '\n",
      "'he chance to'-> ' help her friend the bir'\n",
      "' bag said, \"'-> 'I am sorry, we can came '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>1.01236</td></tr><tr><td>val_loss</td><td>1.05973</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-galaxy-151</strong> at: <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/1pyazj57' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/1pyazj57</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230930_221556-1pyazj57/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CTX_LEN = 12\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "INTERMEDIATE_DIM = EMBEDDING_DIM * 8\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "LR = 0.1\n",
    "\n",
    "TRAIN_TOKENS = 100_000_000\n",
    "\n",
    "\n",
    "LOG_INTERVAL = min(\n",
    "    (min(TRAIN_TOKENS, len(train_data)) // (BATCH_SIZE * CTX_LEN) // 10) + 1, 1000\n",
    ")\n",
    "VALIDATION_INTERVAL = LOG_INTERVAL * 2\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    entity=\"llmnerds\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"ctx\": CTX_LEN,\n",
    "    },\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"embedding\": torch.randn((EMBEDDING_DIM, len(chars)), device=device),\n",
    "    \"w1\": torch.randn((INTERMEDIATE_DIM, EMBEDDING_DIM * CTX_LEN), device=device),\n",
    "    \"b1\": torch.randn((INTERMEDIATE_DIM,), device=device),\n",
    "    \"w2\": torch.randn((EMBEDDING_DIM, INTERMEDIATE_DIM), device=device),\n",
    "    \"b2\": torch.randn((EMBEDDING_DIM,), device=device),\n",
    "    # \"classifier\": torch.randn(\n",
    "    #     (INTERMEDIATE_DIM, len(chars)), device=device, requires_grad=True\n",
    "    # ),\n",
    "}\n",
    "\n",
    "# # glorot init\n",
    "for p in model_params.values():\n",
    "    if len(p.shape) == 2:\n",
    "        torch.nn.init.kaiming_normal_(p)\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "i = 1\n",
    "total_loss = 0\n",
    "val_total_loss = 0\n",
    "\n",
    "token_count = 0\n",
    "optim = torch.optim.Adam(model_params.values(), lr=1e-3)\n",
    "\n",
    "# batch_gen = get_batch(train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "with run:\n",
    "    while token_count < TRAIN_TOKENS:\n",
    "        X, y = get_batch(train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        token_count = i * BATCH_SIZE * CTX_LEN\n",
    "\n",
    "        preds = model(params=model_params, input_ids=X, vocab_size=len(chars))\n",
    "        loss = torch.nn.functional.cross_entropy(input=preds, target=y[:, -1])\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # for param in model_params.values():\n",
    "            #     param -= LR * param.grad\n",
    "            #     param.grad.zero_()\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"loss\": total_loss / LOG_INTERVAL,\n",
    "                    \"epoch\": token_count // len(train_data),\n",
    "                },\n",
    "                step=token_count,\n",
    "            )\n",
    "            total_loss = 0\n",
    "\n",
    "        if i % VALIDATION_INTERVAL == 0:\n",
    "            j = 0\n",
    "            for X_val, y_val in get_epoch(\n",
    "                val_data, ctx_len=CTX_LEN, batch_size=4096, shuffle=False\n",
    "            ):\n",
    "                X_val = X_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = model(\n",
    "                        params=model_params, input_ids=X_val, vocab_size=len(chars)\n",
    "                    )\n",
    "                    loss = torch.nn.functional.cross_entropy(preds, y_val[:, -1])\n",
    "                    val_total_loss += loss.item()\n",
    "                    j += 1\n",
    "            wandb.log({\"val_loss\": val_total_loss / j}, step=token_count, commit=True)\n",
    "\n",
    "            prompts = get_batch(val_data, ctx_len=CTX_LEN, batch_size=5)[0].to(device)\n",
    "            generated = generate(\n",
    "                model=model,\n",
    "                model_params=model_params,\n",
    "                encoded_prompt=prompts,\n",
    "                ctx_len=CTX_LEN,\n",
    "                n_tokens=CTX_LEN * 2,\n",
    "            )\n",
    "            for p in generated:\n",
    "                char_list = p.tolist()\n",
    "                pre_prompt = char_list[:CTX_LEN]\n",
    "                post_prompt = char_list[CTX_LEN:]\n",
    "\n",
    "                print(f\"{repr(decode(pre_prompt))}-> {repr(decode(post_prompt))}\")\n",
    "\n",
    "            val_total_loss = 0\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = get_batch(val_data, ctx_len=CTX_LEN, batch_size=5)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'er! \\nThe nex'-> \"t day, Mian's mommy was excited the juice. They always. One day, who had a special bying. They had lottle joge again. The smy loved the boy came. After to the hole around again.\\nOnce upon a time, heavy, Timmy saw tight. A fincy only the world and lishes. She knew it to all her longerded to open the \"\n",
      "'yons and sta'-> 'y of a happy\\'s toys asked toys to their can. From that day on, every dog! The cat was very sad. Her mom said, happy tow excited for a wards selfush an inside to be needet. Lily went to help other look promise toy cursy for is.\\nOne day, she would little crreful, \"We ble crying. They saw that the pine'\n",
      "'hing unexpec'-> 'ted. It is tho lottle boy. He was excited to see wear appined to see Pally learn and went to the pandow. This yag cats. She glapped her laughed. She wolded Sue. Sue said, \"No, so not having to marccold not started Tom and his differents the speeper- and they all their stickets and them a pudding on '\n",
      "'ded to keep '-> 'behieset and nover again. \\nMr. Sue\\'s help you fighting on to stoodly. Hey created to man they fox them commore saw the moft powner.\\n\"He\\'s mom came over to play anytor in the farthat they letter. Suddenly, a bird night buffel them rest of the big the backyard.\\nOnce upon a time, in the decided to play'\n",
      "'ned the last'-> \"effinishes.\\nThey lived the sun appily with his friends chair now ball. The little fire the mar. They throken for sweech. Let's sittle any gone. \\nAnd so, their car best, it was dark to daye, the fead if he way. Spot went to the patch on the grounds again. One day, he found a beautiful for yountil, bu\"\n"
     ]
    }
   ],
   "source": [
    "generated = generate(\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    encoded_prompt=prompts,\n",
    "    ctx_len=CTX_LEN,\n",
    "    n_tokens=300,\n",
    "    temperature=1,\n",
    "    top_k=50,\n",
    ")\n",
    "for p in generated:\n",
    "    char_list = p.tolist()\n",
    "    pre_prompt = char_list[:CTX_LEN]\n",
    "    post_prompt = char_list[CTX_LEN:]\n",
    "\n",
    "    print(f\"{repr(decode(pre_prompt))}-> {repr(decode(post_prompt))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "d = json.load(open(\"tmp/data00.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': \"Once upon a time, there was a cute puppy named Max. Max was very adorable with his big, brown eyes and wagging tail. One day, Max's owner, Emily, told him that they needed to go to the post office to mail a letter. Max didn't know what that meant, but he was excited to go for a car ride.\\nAt the post office, Emily gave the letter to the nice lady behind the desk. The lady asked Emily for a number and Emily gave her one. Max didn't know what a number was, but he saw the lady type something on the computer.\\nAfter they mailed the letter, Emily and Max went back to the car. Max was happy that they went on an adventure and he couldn't wait for the next one.\",\n",
       " 'instruction': {'prompt:': 'Write a short story (3-5 paragraphs) which only uses very simple words that a 3 year old child would understand. In the story, try to at some point use the verb \"mail\", the noun \"number\" and the adjective \"adorable\". Remember to only use simple words!',\n",
       "  'words': ['mail', 'number', 'adorable'],\n",
       "  'features': []},\n",
       " 'summary': 'Max goes on a car ride with his owner to the post office and watches her mail a letter.',\n",
       " 'source': 'GPT-3.5'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
