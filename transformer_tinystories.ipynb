{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_tensors.patch import monkey_patch\n",
    "\n",
    "monkey_patch()\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the tiny stories dataset\n",
    "\n",
    "# TS_PATH = \"./ts/\"\n",
    "# TS_PATH = Path(TS_PATH)\n",
    "\n",
    "\n",
    "# gpt35_stories = []\n",
    "# gpt4_stories = []\n",
    "\n",
    "import unidecode\n",
    "\n",
    "# for file in tqdm(list(sorted(os.listdir(TS_PATH)))):\n",
    "#     if file.endswith(\".json\"):\n",
    "#         with open(TS_PATH / file, \"r\") as f:\n",
    "#             data = json.load(f)\n",
    "#             for d in data:\n",
    "#                 story = d[\"story\"]\n",
    "#                 if not all(ord(c) < 128 for c in story):\n",
    "#                     story = unidecode.unidecode(story)\n",
    "\n",
    "#                 if d[\"source\"] == \"GPT-3.5\":\n",
    "#                     gpt35_stories.append(story)\n",
    "#                 elif d[\"source\"] == \"GPT-4\":\n",
    "#                     gpt4_stories.append(story)\n",
    "\n",
    "# with open(\"gpt35_stories.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(gpt35_stories))\n",
    "\n",
    "# with open(\"gpt4_stories.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\".join(gpt4_stories))\n",
    "\n",
    "with open(\"gpt35_stories.txt\", \"r\") as f:\n",
    "    gpt35_stories = f.readlines()\n",
    "\n",
    "with open(\"gpt4_stories.txt\", \"r\") as f:\n",
    "    gpt4_stories = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = \"\\n\".join(gpt35_stories + gpt4_stories)\n",
    "data = \"\\n\".join(gpt35_stories[:100_000])\n",
    "\n",
    "del gpt35_stories\n",
    "del gpt4_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "\n",
    "\n",
    "def decode(l):\n",
    "    return \"\".join(\n",
    "        [itos[i] for i in l]\n",
    "    )  # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n !\"$\\'()+,-.0123456789:?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',\n",
       " 76)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(chars), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_data = encode(data)\n",
    "train_data = data[: int(len(data)) - 200_000]\n",
    "val_data = data[int(len(data)) - 200_000 :]\n",
    "del data\n",
    "\n",
    "# train_data = torch.tensor(data)\n",
    "# val_data = torch.tensor(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(data, ctx):\n",
    "    # i = random.randint(0, len(data) - ctx - 1)\n",
    "    i = 0\n",
    "    while i + ctx < len(data):\n",
    "        src = data[i : i + ctx]\n",
    "        dst = data[i + 1 : i + ctx + 1]\n",
    "        yield torch.tensor(encode(src)), torch.tensor(encode(dst))\n",
    "        i += ctx\n",
    "\n",
    "\n",
    "def get_epoch(data, ctx_len, batch_size, shuffle=True):\n",
    "    \"\"\"Yields a tuple of tensors of shape (batch_size, ctx).\n",
    "    X, shape=B C\n",
    "    y, shape=B C\n",
    "    \"\"\"\n",
    "\n",
    "    items = get_item(data, ctx_len)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            X, y = zip(*[next(items) for _ in range(batch_size)])\n",
    "            yield torch.stack(X), torch.stack(y)\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[2, 5] i64 n=10 x∈[1, 63] μ=42.300 σ=22.647 [[38, 63, 52, 54, 1], [54, 1, 57, 50, 53]],\n",
       " tensor[2, 5] i64 n=10 x∈[1, 70] μ=40.200 σ=27.668 [[63, 52, 54, 1, 70], [1, 57, 50, 53, 1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_random_item(data, ctx):\n",
    "    i = random.randint(0, len(data) - ctx - 1)\n",
    "\n",
    "    src = data[i : i + ctx]\n",
    "    dst = data[i + 1 : i + ctx + 1]\n",
    "\n",
    "    return torch.tensor(encode(src)), torch.tensor(encode(dst))\n",
    "\n",
    "\n",
    "def get_batch(data, ctx_len, batch_size, shuffle=True):\n",
    "    \"\"\"Yields a tuple of tensors of shape (batch_size, ctx).\n",
    "    X, shape=B C\n",
    "    y, shape=B C\n",
    "    \"\"\"\n",
    "\n",
    "    batch = [get_random_item(data, ctx_len) for _ in range(batch_size)]\n",
    "    X, y = zip(*batch)\n",
    "\n",
    "    return torch.stack(X), torch.stack(y)\n",
    "\n",
    "\n",
    "get_batch(train_data[:100], ctx_len=5, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTX_LEN = 12\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "X, y = get_batch(train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[4096, 76] n=311296 (1.2Mb) x∈[-36.536, 43.814] μ=0.460 σ=12.011 cuda:0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def attention_block(params, input_ids, vocab_size):\n",
    "\n",
    "\n",
    "def make_params(embedding_dim, ctx_len, vocab_size, layers, device):\n",
    "    model_params = {\n",
    "        \"embedding\": torch.randn((vocab_size, embedding_dim), device=device),\n",
    "        \"pos_enc\": torch.randn((embedding_dim, ctx_len), device=device),\n",
    "        \"blocks\": [],\n",
    "    }\n",
    "\n",
    "    for _ in range(layers):\n",
    "        block = {\n",
    "            \"w_ln1\": torch.ones((embedding_dim), device=device),\n",
    "            \"b_ln1\": torch.zeros((embedding_dim,), device=device),\n",
    "            \"w_attn_k\": torch.randn((embedding_dim, embedding_dim), device=device),\n",
    "            \"w_attn_q\": torch.randn((embedding_dim, embedding_dim), device=device),\n",
    "            \"w_attn_v\": torch.randn((embedding_dim, embedding_dim), device=device),\n",
    "            \"w_fc_up\": torch.randn((embedding_dim * 4, embedding_dim), device=device),\n",
    "            \"b_fc_up\": torch.randn((embedding_dim * 4,), device=device),\n",
    "            \"w_fc_down\": torch.randn((embedding_dim, embedding_dim * 4), device=device),\n",
    "            \"b_fc_down\": torch.randn((embedding_dim,), device=device),\n",
    "            \"w_ln2\": torch.ones((embedding_dim), device=device),\n",
    "            \"b_ln2\": torch.zeros((embedding_dim,), device=device),\n",
    "        }\n",
    "        model_params[\"blocks\"].append(block)\n",
    "\n",
    "    model_params[\"w_ln_f\"] = torch.ones((embedding_dim), device=device)\n",
    "    model_params[\"b_ln_f\"] = torch.zeros((embedding_dim,), device=device)\n",
    "\n",
    "    return model_params\n",
    "\n",
    "\n",
    "model_params = make_params(EMBEDDING_DIM, CTX_LEN, len(chars), 2, device)\n",
    "\n",
    "\n",
    "def transformer_block(params, hidden_states):\n",
    "    res1 = hidden_states\n",
    "    # print(f\"{hidden_states=}\")\n",
    "\n",
    "    ln1 = torch.nn.functional.layer_norm(\n",
    "        input=hidden_states,\n",
    "        weight=params[\"w_ln1\"],\n",
    "        bias=params[\"w_ln1\"],\n",
    "        normalized_shape=(hidden_states.shape[-1],),\n",
    "    )\n",
    "\n",
    "    # print(f\"{ln1=}\")\n",
    "    # print(f\"{params['w_attn_k'].T=}\")\n",
    "\n",
    "    attn_k = ln1 @ params[\"w_attn_k\"].T\n",
    "    attn_q = ln1 @ params[\"w_attn_q\"].T\n",
    "    attn_v = ln1 @ params[\"w_attn_v\"].T\n",
    "\n",
    "    # print(f\"{attn_k=}\")\n",
    "    # print(f\"{attn_q=}\")\n",
    "\n",
    "    import math\n",
    "\n",
    "    n_embed = hidden_states.shape[-1]\n",
    "    attn = (attn_q @ attn_k.transpose(-1, -2)) / math.sqrt(n_embed)\n",
    "\n",
    "    attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "\n",
    "    out = attn @ attn_v + res1\n",
    "\n",
    "    res2 = out\n",
    "\n",
    "    ln2 = torch.nn.functional.layer_norm(\n",
    "        input=out,\n",
    "        weight=params[\"w_ln2\"],\n",
    "        bias=params[\"b_ln2\"],\n",
    "        normalized_shape=(hidden_states.shape[-1],),\n",
    "    )\n",
    "\n",
    "    fc_up = ln2 @ params[\"w_fc_up\"].T + params[\"b_fc_up\"]\n",
    "    fc_up = torch.nn.functional.gelu(fc_up)\n",
    "\n",
    "    fc_down = fc_up @ params[\"w_fc_down\"].T + params[\"b_fc_down\"]\n",
    "\n",
    "    return fc_down + res2\n",
    "\n",
    "\n",
    "def model(params, input_ids, vocab_size):\n",
    "    input_ids = input_ids.long()\n",
    "\n",
    "    embeddings = params[\"embedding\"][input_ids]  # N, CTX_LEN, EMBEDDING_DIM\n",
    "    pos_enc = params[\"pos_enc\"].T[: input_ids.shape[1]]\n",
    "\n",
    "    hidden_states = embeddings + pos_enc\n",
    "    # print(embeddings)\n",
    "\n",
    "    for block in params[\"blocks\"]:\n",
    "        hidden_states = transformer_block(block, hidden_states)\n",
    "\n",
    "    ln_f = torch.nn.functional.layer_norm(\n",
    "        input=hidden_states,\n",
    "        weight=params[\"w_ln_f\"],\n",
    "        bias=params[\"b_ln_f\"],\n",
    "        normalized_shape=(hidden_states.shape[-1],),\n",
    "    )\n",
    "\n",
    "    return ln_f[:, -1, :] @ params[\"embedding\"].T\n",
    "\n",
    "\n",
    "res = model(model_params, X, len(chars))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[5, 13] i64 n=65 x∈[0, 74] μ=42.923 σ=27.898 cuda:0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(\n",
    "    model, model_params, ctx_len, encoded_prompt, n_tokens, temperature=0.0, top_k=1\n",
    "):\n",
    "    \"\"\"Generate n_tokens after prompt\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_tokens):\n",
    "            # print(encoded_prompt)\n",
    "            preds = model(\n",
    "                model_params, encoded_prompt[:, -ctx_len:], vocab_size=len(chars)\n",
    "            )\n",
    "            # print(f\"{preds=}\")\n",
    "\n",
    "            # probs = torch.nn.functional.softmax(preds, dim=-1)\n",
    "            # probs = probs.pow(1 / (temperature + 1e-6))\n",
    "\n",
    "            if temperature == 0.0:\n",
    "                next_token = torch.argmax(preds, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                log_probs = torch.nn.functional.log_softmax(preds, dim=-1)\n",
    "                # print(f\"{log_probs=}\")\n",
    "                log_probs = log_probs / (temperature + 1e-6)\n",
    "                probs = torch.exp(log_probs)\n",
    "                print(f\"{probs=}\")\n",
    "\n",
    "                # Implementing top-k sampling\n",
    "                top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "                next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "                next_token = top_k_indices.gather(dim=1, index=next_token)\n",
    "\n",
    "            # print(f\"{next_token=}\")\n",
    "\n",
    "            encoded_prompt = torch.cat([encoded_prompt, next_token], dim=1)\n",
    "    return encoded_prompt\n",
    "\n",
    "\n",
    "generate(model, model_params, CTX_LEN, X[:5], 1, temperature=0.0, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training data: 20,752,699\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the training data: {len(train_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xl0/work/projects/transformers/wandb/run-20231001_010214-19rdessb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llmnerds/my-awesome-project/runs/19rdessb' target=\"_blank\">fiery-gorge-164</a></strong> to <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/19rdessb' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/19rdessb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'te to eat. T'-> \"orhe l dl  o h'     ,   \"\n",
      "'nd they let '-> '  o                     '\n",
      "'wear her apr'-> ' d rod \\n d i a i   r    '\n",
      "'as already s'-> \"'r    ,e  t n     o     \"\n",
      "'hey saw a bi'-> 'o t e                   '\n",
      "'i Jack! How '-> '  n e   a               '\n",
      "'at rushed aw'-> 't    e                  '\n",
      "'his room. He'-> '    n     a             '\n",
      "' come to the'-> '                        '\n",
      "'because she '-> '                        '\n",
      "'\"Hello, litt'-> 'n     a                 '\n",
      "'ake up.\" \\n\\nL'-> 'e                       '\n",
      "'to have his '-> '       a a              '\n",
      "'to shrink! H'-> '  a  e                  '\n",
      "' meowed in a'-> '   a                    '\n",
      "'om that day '-> '  a                     '\n",
      "' delicious, '-> '    a   a               '\n",
      "'at, especial'-> '  n  a  a               '\n",
      "'ng humble, s'-> ' san     o  a           '\n",
      "'crawled out '-> '                        '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb6813606c042608889263775bef113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▂▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>3.11941</td></tr><tr><td>val_loss</td><td>3.17968</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-gorge-164</strong> at: <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/19rdessb' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/19rdessb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231001_010214-19rdessb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CTX_LEN = 12\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "LR = 0.1\n",
    "\n",
    "TRAIN_TOKENS = 10_000_000\n",
    "\n",
    "\n",
    "LOG_INTERVAL = min(\n",
    "    (min(TRAIN_TOKENS, len(train_data)) // (BATCH_SIZE * CTX_LEN) // 10) + 1, 1000\n",
    ")\n",
    "VALIDATION_INTERVAL = LOG_INTERVAL * 2\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    entity=\"llmnerds\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"ctx\": CTX_LEN,\n",
    "    },\n",
    ")\n",
    "\n",
    "model_params = make_params(EMBEDDING_DIM, CTX_LEN, len(chars), 2, device)\n",
    "\n",
    "\n",
    "def get_params(param_dict_or_list_or_tensor) -> list[torch.Tensor]:\n",
    "    params = []\n",
    "    if isinstance(param_dict_or_list_or_tensor, dict):\n",
    "        for v in param_dict_or_list_or_tensor.values():\n",
    "            params.extend(get_params(v))\n",
    "    elif isinstance(param_dict_or_list_or_tensor, list):\n",
    "        for v in param_dict_or_list_or_tensor:\n",
    "            params.extend(get_params(v))\n",
    "    else:  # If it's a tensor\n",
    "        params.append(param_dict_or_list_or_tensor)\n",
    "    return params\n",
    "\n",
    "\n",
    "param_list = get_params(model_params)\n",
    "\n",
    "for p in param_list:\n",
    "    p.requires_grad = True\n",
    "\n",
    "optim = torch.optim.Adam(param_list, lr=1e-3)\n",
    "\n",
    "i = 1\n",
    "total_loss = 0\n",
    "val_total_loss = 0\n",
    "token_count = 0\n",
    "\n",
    "\n",
    "# batch_gen = get_batch(train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "with run:\n",
    "    while token_count < TRAIN_TOKENS:\n",
    "        X, y = get_batch(train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        token_count = i * BATCH_SIZE * CTX_LEN\n",
    "\n",
    "        preds = model(params=model_params, input_ids=X, vocab_size=len(chars))\n",
    "        loss = torch.nn.functional.cross_entropy(input=preds, target=y[:, -1])\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # for param in model_params.values():\n",
    "            #     param -= LR * param.grad\n",
    "            #     param.grad.zero_()\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"loss\": total_loss / LOG_INTERVAL,\n",
    "                    \"epoch\": token_count // len(train_data),\n",
    "                },\n",
    "                step=token_count,\n",
    "            )\n",
    "            total_loss = 0\n",
    "\n",
    "        if i % VALIDATION_INTERVAL == 0:\n",
    "            j = 0\n",
    "            for X_val, y_val in get_epoch(\n",
    "                val_data, ctx_len=CTX_LEN, batch_size=4096, shuffle=False\n",
    "            ):\n",
    "                X_val = X_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = model(\n",
    "                        params=model_params, input_ids=X_val, vocab_size=len(chars)\n",
    "                    )\n",
    "                    loss = torch.nn.functional.cross_entropy(preds, y_val[:, -1])\n",
    "                    val_total_loss += loss.item()\n",
    "                    j += 1\n",
    "            wandb.log({\"val_loss\": val_total_loss / j}, step=token_count, commit=True)\n",
    "\n",
    "            prompts = get_batch(val_data, ctx_len=CTX_LEN, batch_size=5)[0].to(device)\n",
    "            generated = generate(\n",
    "                model=model,\n",
    "                model_params=model_params,\n",
    "                encoded_prompt=prompts,\n",
    "                ctx_len=CTX_LEN,\n",
    "                n_tokens=CTX_LEN * 2,\n",
    "            )\n",
    "            for p in generated:\n",
    "                char_list = p.tolist()\n",
    "                pre_prompt = char_list[:CTX_LEN]\n",
    "                post_prompt = char_list[CTX_LEN:]\n",
    "\n",
    "                print(f\"{repr(decode(pre_prompt))}-> {repr(decode(post_prompt))}\")\n",
    "\n",
    "            val_total_loss = 0\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = get_batch(val_data, ctx_len=CTX_LEN, batch_size=5)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'er! \\nThe nex'-> \"t day, Mian's mommy was excited the juice. They always. One day, who had a special bying. They had lottle joge again. The smy loved the boy came. After to the hole around again.\\nOnce upon a time, heavy, Timmy saw tight. A fincy only the world and lishes. She knew it to all her longerded to open the \"\n",
      "'yons and sta'-> 'y of a happy\\'s toys asked toys to their can. From that day on, every dog! The cat was very sad. Her mom said, happy tow excited for a wards selfush an inside to be needet. Lily went to help other look promise toy cursy for is.\\nOne day, she would little crreful, \"We ble crying. They saw that the pine'\n",
      "'hing unexpec'-> 'ted. It is tho lottle boy. He was excited to see wear appined to see Pally learn and went to the pandow. This yag cats. She glapped her laughed. She wolded Sue. Sue said, \"No, so not having to marccold not started Tom and his differents the speeper- and they all their stickets and them a pudding on '\n",
      "'ded to keep '-> 'behieset and nover again. \\nMr. Sue\\'s help you fighting on to stoodly. Hey created to man they fox them commore saw the moft powner.\\n\"He\\'s mom came over to play anytor in the farthat they letter. Suddenly, a bird night buffel them rest of the big the backyard.\\nOnce upon a time, in the decided to play'\n",
      "'ned the last'-> \"effinishes.\\nThey lived the sun appily with his friends chair now ball. The little fire the mar. They throken for sweech. Let's sittle any gone. \\nAnd so, their car best, it was dark to daye, the fead if he way. Spot went to the patch on the grounds again. One day, he found a beautiful for yountil, bu\"\n"
     ]
    }
   ],
   "source": [
    "generated = generate(\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    encoded_prompt=prompts,\n",
    "    ctx_len=CTX_LEN,\n",
    "    n_tokens=300,\n",
    "    temperature=1,\n",
    "    top_k=50,\n",
    ")\n",
    "for p in generated:\n",
    "    char_list = p.tolist()\n",
    "    pre_prompt = char_list[:CTX_LEN]\n",
    "    post_prompt = char_list[CTX_LEN:]\n",
    "\n",
    "    print(f\"{repr(decode(pre_prompt))}-> {repr(decode(post_prompt))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "d = json.load(open(\"tmp/data00.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(u) -> list[torch.Tensor]:  # u: param_dict_or_list_or_tensor\n",
    "    flatten = lambda x: [item for sublist in x for item in sublist]\n",
    "    return (\n",
    "        flatten(u.values())\n",
    "        if isinstance(u, dict)\n",
    "        else flatten(u)\n",
    "        if isinstance(u, list)\n",
    "        else [u]\n",
    "    )\n",
    "\n",
    "\n",
    "param_list = get_params(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor[128] x∈[-2.123, 2.940] μ=-0.047 σ=1.024 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.712, 2.628] μ=-0.039 σ=1.006 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.249, 2.290] μ=-0.032 σ=0.969 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.941, 2.137] μ=-0.017 σ=0.951 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.924, 3.264] μ=0.044 σ=1.083 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.790, 1.974] μ=0.044 σ=1.073 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.425, 2.174] μ=0.008 σ=0.819 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.192, 2.293] μ=0.026 σ=0.961 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.331, 3.046] μ=0.056 σ=0.947 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.999, 2.812] μ=-0.030 σ=1.109 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.824, 2.423] μ=-0.045 σ=0.981 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.368, 2.735] μ=-0.104 σ=0.840 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.499, 2.802] μ=0.011 σ=1.023 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.988, 2.761] μ=-0.065 σ=1.063 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.727, 3.332] μ=0.022 σ=1.143 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.324, 3.429] μ=-0.086 σ=1.053 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.810, 2.289] μ=-0.041 σ=1.019 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.134, 3.014] μ=0.097 σ=0.932 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.613, 2.951] μ=-0.085 σ=1.069 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-3.023, 2.318] μ=-0.058 σ=1.004 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.367, 2.360] μ=-0.055 σ=0.942 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.607, 2.714] μ=-0.069 σ=0.965 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.594, 2.407] μ=0.098 σ=0.991 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-3.190, 2.321] μ=0.095 σ=1.018 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-1.956, 2.589] μ=0.148 σ=1.068 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.508, 3.449] μ=-0.024 σ=1.032 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.513, 2.761] μ=0.058 σ=0.979 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.714, 2.319] μ=-0.049 σ=1.082 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.860, 2.714] μ=-0.073 σ=1.051 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.245, 2.544] μ=-0.020 σ=1.036 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.595, 2.756] μ=-0.001 σ=1.047 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.732, 3.247] μ=-0.116 σ=1.064 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.252, 2.356] μ=-0.023 σ=0.955 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.605, 3.073] μ=0.002 σ=0.992 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.661, 2.351] μ=-0.053 σ=1.012 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.163, 1.745] μ=-0.025 σ=0.854 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-3.343, 2.640] μ=0.056 σ=1.056 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.320, 2.310] μ=-0.006 σ=0.966 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.864, 2.752] μ=0.064 σ=1.029 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.375, 2.566] μ=-0.007 σ=0.953 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.786, 2.280] μ=-0.006 σ=1.000 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.101, 2.652] μ=0.034 σ=0.976 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.788, 2.073] μ=-0.076 σ=0.934 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.054, 3.113] μ=0.074 σ=1.082 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.209, 2.406] μ=0.125 σ=0.983 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-3.037, 2.707] μ=0.084 σ=1.116 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-1.675, 2.107] μ=0.063 σ=0.803 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.007, 3.287] μ=0.137 σ=1.031 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-3.057, 3.092] μ=-0.066 σ=0.988 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-3.080, 2.733] μ=-0.108 σ=1.033 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.793, 2.344] μ=0.049 σ=1.040 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.645, 2.611] μ=0.004 σ=0.968 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.814, 2.281] μ=-0.070 σ=0.983 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.097, 1.750] μ=-0.039 σ=0.870 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.101, 2.316] μ=-0.042 σ=0.978 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.174, 2.939] μ=0.120 σ=1.021 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.635, 3.204] μ=-0.042 σ=0.968 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.793, 1.943] μ=0.029 σ=0.944 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.354, 2.119] μ=0.080 σ=0.880 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.139, 2.922] μ=0.084 σ=0.998 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-3.976, 2.313] μ=-0.004 σ=1.169 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.899, 2.305] μ=-0.091 σ=1.013 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.551, 2.178] μ=0.006 σ=0.924 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-3.542, 2.605] μ=0.257 σ=1.039 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.196, 3.318] μ=0.118 σ=1.058 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.635, 2.547] μ=0.075 σ=0.919 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.351, 3.001] μ=-0.020 σ=1.000 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.413, 2.628] μ=0.074 σ=0.998 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.399, 2.072] μ=0.017 σ=0.912 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.580, 2.119] μ=-0.077 σ=1.006 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.634, 2.485] μ=-0.003 σ=0.944 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.557, 2.676] μ=0.003 σ=0.974 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.288, 2.583] μ=0.017 σ=0.946 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.919, 2.442] μ=-0.110 σ=1.099 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.352, 2.042] μ=-0.013 σ=0.955 grad UnbindBackward0 cuda:0,\n",
       " tensor[128] x∈[-2.574, 2.187] μ=0.009 σ=1.007 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.094, 1.485] μ=0.037 σ=0.780 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.416, 1.421] μ=-0.068 σ=1.028 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.798, 2.054] μ=0.494 σ=0.877 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.935, 1.831] μ=0.273 σ=1.262 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.477, 1.107] μ=-0.050 σ=0.760 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.690, 2.137] μ=0.325 σ=1.561 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.084, 2.577] μ=0.252 σ=1.375 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.510, 1.153] μ=-0.050 σ=0.987 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.322, 1.276] μ=0.110 σ=0.741 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.115, 1.571] μ=0.188 σ=0.709 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.216, 2.421] μ=-0.524 σ=1.361 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.600, 2.117] μ=0.377 σ=0.975 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.384, 1.850] μ=-0.271 σ=1.314 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.595, 1.752] μ=0.224 σ=0.910 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.531, 0.791] μ=-0.671 σ=0.898 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.177, 1.332] μ=-0.031 σ=0.792 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.171, 2.830] μ=0.108 σ=1.110 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.209, 2.399] μ=0.532 σ=1.007 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.115, 1.952] μ=0.276 σ=0.935 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.620, 0.999] μ=-0.142 σ=0.932 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.524, 1.620] μ=-0.062 σ=0.826 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.466, 1.898] μ=0.188 σ=1.157 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.432, 2.021] μ=-0.130 σ=0.930 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.744, 1.610] μ=-0.181 σ=1.151 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.219, 1.801] μ=0.205 σ=1.071 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.470, 1.525] μ=-0.120 σ=0.891 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.194, 1.802] μ=0.068 σ=1.355 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.354, 1.933] μ=-0.055 σ=0.918 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.908, 1.144] μ=-0.226 σ=1.171 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.146, 1.232] μ=-0.176 σ=0.761 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.498, 1.733] μ=-0.261 σ=0.991 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.807, 2.256] μ=-0.120 σ=1.090 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.901, 1.354] μ=-0.531 σ=0.914 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.419, 2.759] μ=0.403 σ=1.286 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.811, 2.136] μ=0.247 σ=1.059 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.801, 2.699] μ=-0.049 σ=1.604 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.657, 0.956] μ=-0.412 σ=0.854 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.187, 1.880] μ=-0.070 σ=0.927 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.033, 1.395] μ=-0.435 σ=1.042 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.114, 2.467] μ=0.470 σ=1.102 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.498, 1.221] μ=-0.064 σ=0.851 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.492, 2.171] μ=0.484 σ=1.275 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.294, 1.336] μ=-0.151 σ=0.868 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.205, 2.077] μ=-0.283 σ=1.025 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.447, 2.161] μ=0.429 σ=1.388 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.251, 2.467] μ=0.111 σ=0.980 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.267, 2.052] μ=0.291 σ=0.963 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.678, 1.918] μ=0.000 σ=1.034 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.591, 2.501] μ=0.436 σ=1.136 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.095, 1.572] μ=-0.164 σ=1.272 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.527, 1.883] μ=0.357 σ=0.705 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.401, 0.785] μ=-0.461 σ=1.032 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.100, 1.578] μ=-0.030 σ=1.160 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.698, 1.803] μ=0.149 σ=1.201 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.887, 1.386] μ=-0.195 σ=0.896 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.452, 1.089] μ=0.007 σ=1.111 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.419, 2.677] μ=0.461 σ=1.155 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.320, 1.573] μ=-0.054 σ=0.895 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.286, 2.052] μ=0.328 σ=0.983 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.079, 1.219] μ=-0.092 σ=0.564 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.665, 1.190] μ=0.079 σ=0.821 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.797, 1.597] μ=-0.666 σ=1.032 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.443, 3.037] μ=0.476 σ=1.185 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.286, 2.142] μ=0.017 σ=0.893 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.280, 1.168] μ=0.061 σ=0.755 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.055, 1.100] μ=-0.089 σ=1.066 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.709, 2.909] μ=0.494 σ=1.252 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.331, 2.487] μ=0.209 σ=1.138 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.475, 2.525] μ=-0.066 σ=1.220 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.196, 1.473] μ=0.190 σ=0.866 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.071, 1.328] μ=0.287 σ=0.811 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.113, 1.309] μ=0.158 σ=1.001 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.410, 1.348] μ=0.183 σ=0.810 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.126, 1.802] μ=0.093 σ=0.938 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.420, 1.797] μ=0.423 σ=1.082 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.477, 2.897] μ=1.018 σ=1.104 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.121, 1.643] μ=0.465 σ=0.878 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.870, 1.718] μ=-0.314 σ=1.060 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.338, 1.832] μ=0.506 σ=1.188 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.265, 0.957] μ=-0.116 σ=0.662 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.023, 2.609] μ=0.134 σ=0.968 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.974, 1.230] μ=-0.422 σ=0.994 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.806, 1.464] μ=-0.147 σ=0.928 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.459, 1.605] μ=-0.219 σ=0.980 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.262, 2.401] μ=0.334 σ=1.062 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.792, 1.147] μ=-0.167 σ=0.931 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.263, 1.672] μ=0.361 σ=0.863 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.546, 2.222] μ=0.434 σ=1.153 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.975, 1.846] μ=0.166 σ=1.192 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.099, 2.560] μ=0.341 σ=1.085 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.884, 2.199] μ=-0.154 σ=1.131 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.256, 1.384] μ=-0.034 σ=0.851 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.686, 2.711] μ=0.284 σ=0.936 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.346, 1.658] μ=0.193 σ=0.989 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.927, 0.706] μ=-0.370 σ=0.879 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.019, 0.693] μ=-0.569 σ=0.781 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.178, 2.246] μ=-0.154 σ=1.439 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.922, 1.672] μ=-0.077 σ=0.955 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.135, 1.131] μ=-0.486 σ=1.054 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.507, 1.053] μ=0.042 σ=0.853 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.429, 2.227] μ=0.124 σ=1.074 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.210, 1.988] μ=-0.020 σ=1.226 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.798, 0.968] μ=0.050 σ=0.460 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.582, 2.695] μ=0.428 σ=0.880 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.193, 2.219] μ=0.341 σ=1.012 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.466, 1.737] μ=0.268 σ=0.942 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.900, 1.421] μ=0.036 σ=0.995 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.360, 1.081] μ=-0.070 σ=0.721 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.755, 1.624] μ=-0.289 σ=0.960 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.878, 1.985] μ=0.370 σ=1.523 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.268, 1.143] μ=-0.015 σ=0.775 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-2.890, 1.866] μ=-0.256 σ=1.214 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.177, 1.655] μ=-0.030 σ=0.991 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.392, 2.324] μ=0.114 σ=1.057 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.756, 1.982] μ=0.470 σ=0.777 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.105, 2.373] μ=0.013 σ=1.132 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.786, 0.964] μ=-0.360 σ=0.825 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.742, 2.020] μ=0.212 σ=1.024 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.313, 1.935] μ=0.287 σ=0.994 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.105, 1.883] μ=0.072 σ=1.006 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.048, 3.304] μ=0.274 σ=1.142 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.303, 1.187] μ=0.300 σ=0.745 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.996, 1.695] μ=-0.181 σ=1.091 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.632, 1.644] μ=0.045 σ=1.024 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.395, 1.431] μ=0.103 σ=0.866 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.413, 1.256] μ=0.477 σ=0.518 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-1.233, 1.289] μ=-0.315 σ=0.678 grad UnbindBackward0 cuda:0,\n",
       " tensor[12] x∈[-0.662, 1.567] μ=0.401 σ=0.684 grad UnbindBackward0 cuda:0,\n",
       " {'w_ln1': tensor[128] x∈[0.998, 1.002] μ=1.000 σ=0.002 grad cuda:0,\n",
       "  'b_ln1': tensor[128] \u001b[38;2;127;127;127mall_zeros\u001b[0m grad cuda:0,\n",
       "  'w_attn_k': tensor[128, 128] n=16384 (64Kb) x∈[-4.283, 3.451] μ=-0.001 σ=1.000 grad cuda:0,\n",
       "  'w_attn_q': tensor[128, 128] n=16384 (64Kb) x∈[-3.940, 3.747] μ=0.005 σ=0.999 grad cuda:0,\n",
       "  'w_attn_v': tensor[128, 128] n=16384 (64Kb) x∈[-4.517, 3.907] μ=-0.002 σ=1.000 grad cuda:0,\n",
       "  'w_fc_up': tensor[512, 128] n=65536 (0.2Mb) x∈[-4.203, 4.062] μ=-0.003 σ=1.000 grad cuda:0,\n",
       "  'b_fc_up': tensor[512] 2Kb x∈[-2.629, 3.337] μ=-0.035 σ=0.965 grad cuda:0,\n",
       "  'w_fc_down': tensor[128, 512] n=65536 (0.2Mb) x∈[-4.135, 4.489] μ=0.006 σ=0.999 grad cuda:0,\n",
       "  'b_fc_down': tensor[128] x∈[-2.344, 3.346] μ=-0.029 σ=0.996 grad cuda:0,\n",
       "  'w_ln2': tensor[128] x∈[0.998, 1.002] μ=1.000 σ=0.002 grad cuda:0,\n",
       "  'b_ln2': tensor[128] x∈[-0.002, 0.002] μ=1.616e-05 σ=0.002 grad cuda:0},\n",
       " {'w_ln1': tensor[128] x∈[0.998, 1.002] μ=1.000 σ=0.002 grad cuda:0,\n",
       "  'b_ln1': tensor[128] \u001b[38;2;127;127;127mall_zeros\u001b[0m grad cuda:0,\n",
       "  'w_attn_k': tensor[128, 128] n=16384 (64Kb) x∈[-3.616, 3.644] μ=-0.008 σ=1.000 grad cuda:0,\n",
       "  'w_attn_q': tensor[128, 128] n=16384 (64Kb) x∈[-3.874, 3.787] μ=-0.003 σ=0.991 grad cuda:0,\n",
       "  'w_attn_v': tensor[128, 128] n=16384 (64Kb) x∈[-3.887, 4.047] μ=-0.005 σ=0.997 grad cuda:0,\n",
       "  'w_fc_up': tensor[512, 128] n=65536 (0.2Mb) x∈[-4.360, 4.108] μ=-0.004 σ=0.997 grad cuda:0,\n",
       "  'b_fc_up': tensor[512] 2Kb x∈[-3.378, 2.665] μ=0.003 σ=0.995 grad cuda:0,\n",
       "  'w_fc_down': tensor[128, 512] n=65536 (0.2Mb) x∈[-4.597, 3.931] μ=-0.000 σ=0.999 grad cuda:0,\n",
       "  'b_fc_down': tensor[128] x∈[-3.319, 2.947] μ=0.057 σ=0.910 grad cuda:0,\n",
       "  'w_ln2': tensor[128] x∈[0.998, 1.002] μ=1.000 σ=0.002 grad cuda:0,\n",
       "  'b_ln2': tensor[128] x∈[-0.002, 0.002] μ=-0.000 σ=0.002 grad cuda:0},\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.999,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 1.000,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.999,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 1.000,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.001,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.999,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.999,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 1.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 0.998,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.001,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.001,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.001,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.000,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.001,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.000,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.000,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.001,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.001,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.001,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 -0.002,\n",
       " tensor grad UnbindBackward0 cuda:0 0.002]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
