{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from lovely_tensors.patch import monkey_patch; monkey_patch()\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def new_gelu(input):\n",
    "    return (\n",
    "        0.5\n",
    "        * input\n",
    "        * (\n",
    "            1.0\n",
    "            + torch.tanh(\n",
    "                math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1d(x, weight, bias=None):\n",
    "    size_out = x.size()[:-1] + (weight.size(-1),)\n",
    "    x = torch.addmm(bias, x.view(-1, x.size(-1)), weight)\n",
    "    x = x.view(size_out)\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_block(i, input_hidden_state, model_state):\n",
    "    def block_state(key):\n",
    "        return model_state[f\"h.{i}.{key}\"]\n",
    "\n",
    "    def attention_state(key):\n",
    "        return model_state[f\"h.{i}.attn.{key}\"]\n",
    "\n",
    "    # attention block\n",
    "    ln1 = torch.nn.functional.layer_norm(\n",
    "        input=input_hidden_state,\n",
    "        weight=block_state(\"ln_1.weight\"),\n",
    "        bias=block_state(\"ln_1.bias\"),\n",
    "        normalized_shape=(768,),\n",
    "    )\n",
    "\n",
    "    w_q, w_k, w_v = attention_state(\"c_attn.weight\").chunk(3, dim=1)\n",
    "    b_q, b_k, b_v = attention_state(\"c_attn.bias\").chunk(3, dim=0)\n",
    "\n",
    "    q = conv_1d(ln1, w_q, b_q)\n",
    "    k = conv_1d(ln1, w_k, b_k)\n",
    "    v = conv_1d(ln1, w_v, b_v)\n",
    "\n",
    "    q_chunked = torch.stack(q.chunk(12, dim=-1))\n",
    "    k_chunked = torch.stack(k.chunk(12, dim=-1))\n",
    "    v_chunked = torch.stack(v.chunk(12, dim=-1))\n",
    "\n",
    "    attention = torch.matmul(q_chunked, k_chunked.transpose(-1, -2))\n",
    "\n",
    "    attention_rescaled = attention / (64**0.5)\n",
    "\n",
    "    mask = torch.triu(torch.ones_like(attention_rescaled), diagonal=1).bool()\n",
    "    attention_masked = attention_rescaled.masked_fill(\n",
    "        mask, torch.finfo(torch.float32).min\n",
    "    )\n",
    "\n",
    "    attention_softmaxed = torch.nn.functional.softmax(attention_masked, dim=-1)\n",
    "    attention_output = torch.matmul(attention_softmaxed, v_chunked)\n",
    "\n",
    "    out_tuple = [x[0] for x in attention_output.chunk(12, dim=0)]\n",
    "    combined_attention_output = torch.cat(out_tuple, dim=-1)\n",
    "\n",
    "    w_cproj = attention_state(\"c_proj.weight\")\n",
    "    b_cproj = attention_state(\"c_proj.bias\")\n",
    "\n",
    "    crosstalk = conv_1d(combined_attention_output, w_cproj, b_cproj)\n",
    "    after_residual = crosstalk + input_hidden_state\n",
    "\n",
    "    # mlp block\n",
    "    before_ln2 = after_residual\n",
    "\n",
    "    ln2 = torch.nn.functional.layer_norm(\n",
    "        input=after_residual,\n",
    "        weight=block_state(\"ln_2.weight\"),\n",
    "        bias=block_state(\"ln_2.bias\"),\n",
    "        normalized_shape=(768,),\n",
    "    )\n",
    "\n",
    "    w_fc = block_state(\"mlp.c_fc.weight\")\n",
    "    b_fc = block_state(\"mlp.c_fc.bias\")\n",
    "\n",
    "    after_up = conv_1d(ln2, w_fc, b_fc)\n",
    "    activated = new_gelu(after_up)\n",
    "\n",
    "    w_proj = block_state(\"mlp.c_proj.weight\")\n",
    "    b_proj = block_state(\"mlp.c_proj.bias\")\n",
    "\n",
    "    after_down = conv_1d(activated, w_proj, b_proj)\n",
    "\n",
    "    after_residual_2 = after_down + before_ln2\n",
    "\n",
    "    return after_residual_2\n",
    "\n",
    "\n",
    "def transformer(token_ids, model_state):\n",
    "    token_embeddings = model_state[\"wte.weight\"][token_ids]\n",
    "    positions = torch.arange(len(token_ids))  # [0,1,2,3...]\n",
    "    position_embeddings = model_state[\"wpe.weight\"][positions]\n",
    "    embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "    hs = embeddings\n",
    "    for i in range(12):\n",
    "        hs = transformer_block(i, hs, model_state)\n",
    "\n",
    "    ln_w = model_state[\"ln_f.weight\"]\n",
    "    ln_b = model_state[\"ln_f.bias\"]\n",
    "\n",
    "    ln = torch.nn.functional.layer_norm(\n",
    "        input=hs, weight=ln_w, bias=ln_b, normalized_shape=(768,)\n",
    "    )\n",
    "\n",
    "    return ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(token_ids, model_state):\n",
    "    transformer_output = transformer(token_ids, model_state)\n",
    "    logits = torch.matmul(transformer_output, model_state[\"wte.weight\"].T)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "model_params = torch.load(\"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 50257] n=502570 (1.9Mb) x∈[-110.039, -29.114] μ=-76.757 σ=15.804"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = language_model(token_ids, model_params)\n",
    "out\n",
    "\n",
    "# out[0].chans(scale=4, cmap=\"seismic\")\n",
    "# out[1].chans(scale=4, cmap=\"seismic\")\n",
    "# out[2].chans(scale=4, cmap=\"seismic\")\n",
    "# out[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
