{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_tensors.patch import monkey_patch\n",
    "\n",
    "monkey_patch()\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tiny_shakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "\n",
    "\n",
    "def decode(l):\n",
    "    return \"\".join(\n",
    "        [itos[i] for i in l]\n",
    "    )  # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "encoded_data = encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_data[: int(len(encoded_data) * 0.8)]\n",
    "val_data = encoded_data[int(len(encoded_data) * 0.8) :]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_data = torch.tensor(train_data).to(device)\n",
    "val_data = torch.tensor(val_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor[2, 5] i64 n=10 x∈[1, 58] μ=36.300 σ=22.346 cuda:0 [[1, 46, 43, 39, 56], [57, 58, 1, 15, 47]], tensor[2, 5] i64 n=10 x∈[1, 58] μ=36.400 σ=22.451 cuda:0 [[46, 43, 39, 56, 1], [58, 1, 15, 47, 58]])\n",
      "(tensor[2, 5] i64 n=10 x∈[0, 54] μ=33.100 σ=20.058 cuda:0 [[10, 0, 31, 54, 43], [54, 43, 39, 49, 8]], tensor[2, 5] i64 n=10 x∈[0, 54] μ=30.600 σ=20.326 cuda:0 [[0, 31, 54, 43, 39], [43, 39, 49, 8, 0]])\n",
      "(tensor[2, 5] i64 n=10 x∈[0, 64] μ=39.200 σ=25.616 cuda:0 [[63, 1, 44, 59, 56], [64, 43, 52, 10, 0]], tensor[2, 5] i64 n=10 x∈[0, 59] μ=33.700 σ=24.518 cuda:0 [[1, 44, 59, 56, 58], [43, 52, 10, 0, 14]])\n",
      "(tensor[2, 5] i64 n=10 x∈[6, 58] μ=41.900 σ=17.860 cuda:0 [[14, 43, 44, 53, 56], [58, 46, 43, 56, 6]], tensor[2, 5] i64 n=10 x∈[1, 56] μ=39.100 σ=19.519 cuda:0 [[43, 44, 53, 56, 43], [46, 43, 56, 6, 1]])\n",
      "(tensor[2, 5] i64 n=10 x∈[1, 57] μ=30.500 σ=24.959 cuda:0 [[1, 51, 43, 1, 57], [39, 49, 6, 1, 57]], tensor[2, 5] i64 n=10 x∈[1, 57] μ=37.300 σ=24.281 cuda:0 [[51, 43, 1, 57, 54], [49, 6, 1, 57, 54]])\n",
      "(tensor[2, 5] i64 n=10 x∈[41, 64] μ=51.100 σ=7.460 cuda:0 [[58, 47, 64, 43, 52], [54, 56, 53, 41, 43]], tensor[2, 5] i64 n=10 x∈[10, 64] μ=45.200 σ=14.343 cuda:0 [[47, 64, 43, 52, 10], [56, 53, 41, 43, 43]])\n",
      "(tensor[2, 5] i64 n=10 x∈[0, 52] μ=29.000 σ=22.603 cuda:0 [[43, 42, 1, 39, 52], [0, 0, 13, 50, 50]], tensor[2, 5] i64 n=10 x∈[0, 63] μ=32.000 σ=23.542 cuda:0 [[42, 1, 39, 52, 63], [0, 13, 50, 50, 10]])\n",
      "(tensor[2, 5] i64 n=10 x∈[1, 61] μ=38.500 σ=23.287 cuda:0 [[18, 47, 56, 57, 58], [43, 1, 61, 43, 1]], tensor[2, 5] i64 n=10 x∈[1, 61] μ=37.900 σ=25.998 cuda:0 [[47, 56, 57, 58, 1], [1, 61, 43, 1, 54]])\n",
      "(tensor[2, 5] i64 n=10 x∈[0, 58] μ=28.900 σ=24.324 cuda:0 [[0, 0, 18, 47, 56], [1, 15, 47, 58, 47]], tensor[2, 5] i64 n=10 x∈[0, 64] μ=40.900 σ=21.840 cuda:0 [[0, 18, 47, 56, 57], [15, 47, 58, 47, 64]])\n"
     ]
    }
   ],
   "source": [
    "def get_item(data, ctx):\n",
    "    # i = random.randint(0, len(data) - ctx - 1)\n",
    "    i = 0\n",
    "    while i + ctx < len(data):\n",
    "        src = data[i : i + ctx]\n",
    "        dst = data[i + 1 : i + ctx + 1]\n",
    "        yield src, dst\n",
    "        i += ctx\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def get_batch(data, ctx_len, batch_size, shuffle=True):\n",
    "    \"\"\"Yields a tuple of tensors of shape (batch_size, ctx).\n",
    "    X, shape=B C\n",
    "    y, shape=B C\n",
    "    \"\"\"\n",
    "\n",
    "    if shuffle:\n",
    "        i = random.randint(0, batch_size - 1)\n",
    "        items = list(get_item(data[i:], ctx_len))\n",
    "\n",
    "        random.shuffle(items)\n",
    "        iter_items = iter(items)\n",
    "    else:\n",
    "        iter_items = get_item(data, ctx_len)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            X, y = zip(*[next(iter_items) for _ in range(batch_size)])\n",
    "            yield torch.stack(X), torch.stack(y)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "\n",
    "for batch in get_batch(train_data[:100], ctx_len=5, batch_size=2):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_gen = get_batch(train_data, ctx_len=CTX_LEN, batch_size=32)\n",
    "# X, y = next(batch_gen)\n",
    "# with torch.no_grad():\n",
    "#     preds = model(model_params, X, vocab_size=len(chars))\n",
    "#     loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 10] n=100 x∈[-2.417, 3.082] μ=-0.049 σ=1.094"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.randn(10, 10)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 10] n=100 x∈[-0.545, 0.531] μ=-0.032 σ=0.311"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 10] n=100 x∈[-0.545, 0.531] μ=-0.032 σ=0.311"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter[10, 10] n=100 x∈[-0.313, 0.309] μ=-0.013 σ=0.196 grad"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.nn.Linear(10, 10)\n",
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# in_ # BATCH_SIZE x CTX_LEN x len(chars)  (feed in a batch of CTX_LEN embeddings (each is a one-hot-encoded character)\n",
    "# out_ # BATCH_SIZE x len(chars)  (get out a (one-hot-encoded) next-char prediction for each batch-item\n",
    "# l1 = torch.nn.Linear(len(chars) * CTX_LEN, len(chars)).to(device)\n",
    "\n",
    "# model_params = {\n",
    "#     \"embedding\": torch.randn((len(chars), EMBEDDING_DIM), device=device, requires_grad = True),\n",
    "#     # \"w\": torch.randn((EMBEDDING_DIM * CTX_LEN, len(chars)), device=device, requires_grad = True),\n",
    "# }\n",
    "\n",
    "\n",
    "def model(params, input_ids, vocab_size):\n",
    "    \"\"\"This model takes in a sequence and predicts 1 token\"\"\"\n",
    "\n",
    "    one_hot_inputs = torch.nn.functional.one_hot(input_ids, num_classes=vocab_size)\n",
    "    one_hot_inputs = one_hot_inputs.float()\n",
    "\n",
    "    embeddings = one_hot_inputs @ params[\"embedding\"].T  # N, CTX_LEN, EMBEDDING_DIM\n",
    "\n",
    "    # preds = hidden_states[:, -1, :] # @ params[\"w\"]\n",
    "\n",
    "    hidden_state = (\n",
    "        embeddings.view((input_ids.shape[0], -1)) @ params[\"w1\"].T + params[\"b1\"]\n",
    "    )\n",
    "    hidden_state = torch.nn.functional.relu(hidden_state)\n",
    "\n",
    "    hidden_state = (\n",
    "        hidden_state.view((input_ids.shape[0], -1)) @ params[\"w2\"].T + params[\"b2\"]\n",
    "    )\n",
    "    hidden_state = torch.nn.functional.relu(hidden_state)\n",
    "\n",
    "    preds = hidden_state @ params[\"embedding\"]\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# batch_gen = get_batch(train_data, ctx_len=CTX_LEN, batch_size=32)\n",
    "# X, y = next(batch_gen)\n",
    "# with torch.no_grad():\n",
    "#     preds = model(model_params, X, vocab_size=len(chars))\n",
    "#     loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9a6mexqh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5643e0d5ea68415880a839bb90176522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.010 MB of 0.033 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.319777…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>loss</td><td>2.97865</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-salad-113</strong> at: <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/9a6mexqh' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/9a6mexqh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230924_000024-9a6mexqh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9a6mexqh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d033927400bd48f28c9c919b29586685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112229777791072, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xl0/work/projects/transformers/wandb/run-20230924_000103-ehol37bz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llmnerds/my-awesome-project/runs/ehol37bz' target=\"_blank\">dashing-wind-114</a></strong> to <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/ehol37bz' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/ehol37bz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 54\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(batch_gen)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     batch_gen \u001b[39m=\u001b[39m get_batch(\n\u001b[1;32m     57\u001b[0m         train_data, ctx_len\u001b[39m=\u001b[39mCTX_LEN, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[0;32m---> 59\u001b[0m     Y, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(batch_gen)\n\u001b[1;32m     61\u001b[0m token_count \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39m BATCH_SIZE \u001b[39m*\u001b[39m CTX_LEN\n\u001b[1;32m     63\u001b[0m preds \u001b[39m=\u001b[39m model(params\u001b[39m=\u001b[39mmodel_params, input_ids\u001b[39m=\u001b[39mX, vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(chars))\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(data, ctx_len, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m shuffle:\n\u001b[1;32m     21\u001b[0m     i \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, batch_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     items \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(get_item(data[i:], ctx_len))\n\u001b[1;32m     24\u001b[0m     random\u001b[39m.\u001b[39mshuffle(items)\n\u001b[1;32m     25\u001b[0m     iter_items \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(items)\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mget_item\u001b[0;34m(data, ctx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_item\u001b[39m(data, ctx):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# i = random.randint(0, len(data) - ctx - 1)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mwhile\u001b[39;00m i \u001b[39m+\u001b[39m ctx \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39;49m(data):\n\u001b[1;32m      5\u001b[0m         src \u001b[39m=\u001b[39m data[i : i \u001b[39m+\u001b[39m ctx]\n\u001b[1;32m      6\u001b[0m         dst \u001b[39m=\u001b[39m data[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m : i \u001b[39m+\u001b[39m ctx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CTX_LEN = 32\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "INTERMEDIATE_DIM = EMBEDDING_DIM * 8\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "LR = 0.1\n",
    "LOG_INTERVAL = len(train_data) // BATCH_SIZE // 10\n",
    "VALIDATION_INTERVAL = len(train_data) // BATCH_SIZE // 5\n",
    "\n",
    "\n",
    "TRAIN_TOKENS = len(train_data) * 10\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    entity=\"llmnerds\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"ctx\": CTX_LEN,\n",
    "    },\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"embedding\": torch.randn((EMBEDDING_DIM, len(chars)), device=device),\n",
    "    \"w1\": torch.randn((INTERMEDIATE_DIM, EMBEDDING_DIM * CTX_LEN), device=device),\n",
    "    \"b1\": torch.randn((INTERMEDIATE_DIM,), device=device),\n",
    "    \"w2\": torch.randn((EMBEDDING_DIM, INTERMEDIATE_DIM), device=device),\n",
    "    \"b2\": torch.randn((EMBEDDING_DIM,), device=device),\n",
    "    # \"classifier\": torch.randn(\n",
    "    #     (INTERMEDIATE_DIM, len(chars)), device=device, requires_grad=True\n",
    "    # ),\n",
    "}\n",
    "\n",
    "# # glorot init\n",
    "for p in model_params.values():\n",
    "    if len(p.shape) == 2:\n",
    "        torch.nn.init.kaiming_normal_(p)\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "i = 1\n",
    "total_loss = 0\n",
    "val_total_loss = 0\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(model_params.values(), lr=1e-3)\n",
    "\n",
    "batch_gen = get_batch(train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE, shuffle=True)\n",
    "while True: #i < TRAIN_TOKENS:\n",
    "    try:\n",
    "        X, y = next(batch_gen)\n",
    "    except StopIteration:\n",
    "        batch_gen = get_batch(\n",
    "            train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE, shuffle=True\n",
    "        )\n",
    "        Y, y = next(batch_gen)\n",
    "\n",
    "    token_count = i * BATCH_SIZE * CTX_LEN\n",
    "\n",
    "    preds = model(params=model_params, input_ids=X, vocab_size=len(chars))\n",
    "    loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # for param in model_params.values():\n",
    "        #     param -= LR * param.grad\n",
    "        #     param.grad.zero_()\n",
    "\n",
    "\n",
    "    if i % LOG_INTERVAL == 0:\n",
    "        wandb.log(\n",
    "            {\"loss\": total_loss / LOG_INTERVAL, \"epoch\": (token_count) // len(train_data)},\n",
    "            step=token_count,\n",
    "        )\n",
    "        total_loss = 0\n",
    "\n",
    "    if i % VALIDATION_INTERVAL == 0:\n",
    "        j = 0\n",
    "        for X_val, y_val in get_batch(\n",
    "            val_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE, shuffle=False\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                preds = model(params=model_params, input_ids=X_val, vocab_size=len(chars))\n",
    "                loss = torch.nn.functional.cross_entropy(preds, y_val[:, -1])\n",
    "                val_total_loss += loss.item()\n",
    "                j += 1\n",
    "        wandb.log({\"val_loss\": val_total_loss / j}, step=token_count)\n",
    "        val_total_loss = 0\n",
    "    i += 1\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
