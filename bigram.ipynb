{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_tensors.patch import monkey_patch\n",
    "\n",
    "monkey_patch()\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tiny_shakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "\n",
    "\n",
    "def decode(l):\n",
    "    return \"\".join(\n",
    "        [itos[i] for i in l]\n",
    "    )  # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "encoded_data = encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_data[: int(len(encoded_data) * 0.8)]\n",
    "val_data = encoded_data[int(len(encoded_data) * 0.8) :]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_data = torch.tensor(train_data).to(device)\n",
    "val_data = torch.tensor(val_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(data, ctx):\n",
    "    # i = random.randint(0, len(data) - ctx - 1)\n",
    "    i = 0\n",
    "    while i + ctx < len(data):\n",
    "        src = data[i : i + ctx]\n",
    "        dst = data[i + 1 : i + ctx + 1]\n",
    "        yield src, dst\n",
    "        i += ctx\n",
    "\n",
    "import random\n",
    "def get_random_item(data, ctx):\n",
    "    i = random.randint(0, len(data) - ctx - 1)\n",
    "    src = data[i : i + ctx]\n",
    "    dst = data[i + 1 : i + ctx + 1]\n",
    "    yield src, dst\n",
    "\n",
    "def get_batch(data, ctx_len, batch_size):\n",
    "    \"\"\"Yields a tuple of tensors of shape (batch_size, ctx).\n",
    "    X, shape=B C\n",
    "    y, shape=B C\n",
    "    \"\"\"\n",
    "    gen = get_random_item(data, ctx_len)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            X, y = zip(*[next(gen) for _ in range(batch_size)])\n",
    "            yield torch.stack(X), torch.stack(y)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "\n",
    "# for batch in get_batch(train_data[:100], ctx_len=5, batch_size=2):\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_gen = get_batch(train_data, ctx_len=CTX_LEN, batch_size=32)\n",
    "# X, y = next(batch_gen)\n",
    "# with torch.no_grad():\n",
    "#     preds = model(model_params, X, vocab_size=len(chars))\n",
    "#     loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 10] n=100 x∈[-3.179, 2.142] μ=-0.068 σ=0.929"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.randn(10,10)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 10] n=100 x∈[-0.540, 0.543] μ=0.049 σ=0.296"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 10] n=100 x∈[-0.540, 0.543] μ=0.049 σ=0.296"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter[10, 10] n=100 x∈[-0.300, 0.315] μ=-0.009 σ=0.177 grad"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.nn.Linear(10, 10)\n",
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTX_LEN = 1\n",
    "EMBEDDING_DIM = len(chars)\n",
    "\n",
    "\n",
    "# in_ # BATCH_SIZE x CTX_LEN x len(chars)  (feed in a batch of CTX_LEN embeddings (each is a one-hot-encoded character)\n",
    "# out_ # BATCH_SIZE x len(chars)  (get out a (one-hot-encoded) next-char prediction for each batch-item\n",
    "# l1 = torch.nn.Linear(len(chars) * CTX_LEN, len(chars)).to(device)\n",
    "\n",
    "# model_params = {\n",
    "#     \"embedding\": torch.randn((len(chars), EMBEDDING_DIM), device=device, requires_grad = True),\n",
    "#     # \"w\": torch.randn((EMBEDDING_DIM * CTX_LEN, len(chars)), device=device, requires_grad = True),\n",
    "# }\n",
    "\n",
    "# # glorot init\n",
    "# for p in model_params.values():\n",
    "#     # torch.nn.init.kaiming_uniform_(p)\n",
    "    \n",
    "\n",
    "\n",
    "def model(params, input_ids, vocab_size):\n",
    "    \"\"\"This model takes in a sequence and predicts 1 token\"\"\"\n",
    "\n",
    "    one_hot_inputs = torch.nn.functional.one_hot(input_ids, num_classes=vocab_size)\n",
    "    one_hot_inputs = one_hot_inputs.float()\n",
    "\n",
    "    hidden_states = one_hot_inputs @ params[\"embedding\"] # N, CTX_LEN, EMBEDDING_DIM\n",
    "    \n",
    "    # preds = hidden_states[:, -1, :] # @ params[\"w\"]\n",
    "\n",
    "    preds = hidden_states.view((input_ids.shape[0], -1)) @ params[\"w\"]\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# batch_gen = get_batch(train_data, ctx_len=CTX_LEN, batch_size=32)\n",
    "# X, y = next(batch_gen)\n",
    "# with torch.no_grad():\n",
    "#     preds = model(model_params, X, vocab_size=len(chars))\n",
    "#     loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4849d7ac8536485bbccc5924f5c37e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112805566659114, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xl0/work/projects/transformers/wandb/run-20230923_220508-tioybnfh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llmnerds/my-awesome-project/runs/tioybnfh' target=\"_blank\">revived-surf-81</a></strong> to <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/tioybnfh' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/tioybnfh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ea7d24f89845d8afceaf8d7e76503b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NEPOCH):\n\u001b[1;32m     27\u001b[0m     \u001b[39m# shuffle train data\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[39m# train_data = train_data[torch.randperm(train_data.shape[0])]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     train_generator \u001b[39m=\u001b[39m get_batch(train_data, ctx_len\u001b[39m=\u001b[39mCTX_LEN, batch_size\u001b[39m=\u001b[39mBATCH_SIZE)\n\u001b[0;32m---> 32\u001b[0m     \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m tqdm(train_generator):\n\u001b[1;32m     33\u001b[0m         preds \u001b[39m=\u001b[39m model(params\u001b[39m=\u001b[39mmodel_params, input_ids\u001b[39m=\u001b[39mX, vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(chars))\n\u001b[1;32m     34\u001b[0m         loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy(preds, y[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/envs/torch/lib/python3.10/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/torch/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(data, ctx_len, batch_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m         X, y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[\u001b[39mnext\u001b[39m(gen) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size)])\n\u001b[1;32m     27\u001b[0m         \u001b[39myield\u001b[39;00m torch\u001b[39m.\u001b[39mstack(X), torch\u001b[39m.\u001b[39mstack(y)\n\u001b[1;32m     28\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m         X, y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[\u001b[39mnext\u001b[39;49m(gen) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size)])\n\u001b[1;32m     27\u001b[0m         \u001b[39myield\u001b[39;00m torch\u001b[39m.\u001b[39mstack(X), torch\u001b[39m.\u001b[39mstack(y)\n\u001b[1;32m     28\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not an iterator"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "NEPOCH = 10\n",
    "LR = 0.1\n",
    "LOG_INTERVAL = 500\n",
    "VALIDATION_INTERVAL = len(train_data) // BATCH_SIZE // 5\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    entity=\"llmnerds\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"ctx\": CTX_LEN,\n",
    "    },\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"embedding\": torch.randn((len(chars), EMBEDDING_DIM), device=device, requires_grad = True),\n",
    "    \"w\": torch.randn((EMBEDDING_DIM * CTX_LEN, len(chars)), device=device, requires_grad = True),\n",
    "}\n",
    "\n",
    "i = 1\n",
    "total_loss = 0\n",
    "val_total_loss = 0\n",
    "for epoch in range(NEPOCH):\n",
    "    # shuffle train data\n",
    "    # train_data = train_data[torch.randperm(train_data.shape[0])]\n",
    "\n",
    "    train_generator = get_batch(train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE)\n",
    "\n",
    "    for X, y in tqdm(train_generator):\n",
    "        preds = model(params=model_params, input_ids=X, vocab_size=len(chars))\n",
    "        loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param in model_params.values():\n",
    "                param -= LR * param.grad\n",
    "                param.grad.zero_()\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            wandb.log({\"loss\": total_loss / LOG_INTERVAL}, step=i)\n",
    "            total_loss = 0\n",
    "\n",
    "        if i % VALIDATION_INTERVAL == 0:\n",
    "            j = 0\n",
    "            for X, y in get_batch(val_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE * 4):\n",
    "                preds = model(params=model_params, input_ids=X, vocab_size=len(chars))\n",
    "                loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "                val_total_loss += loss.item()\n",
    "                j += 1\n",
    "            wandb.log({\"val_loss\": val_total_loss / j}, step=i)\n",
    "            val_total_loss = 0\n",
    "        i += 1\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
