{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxl0\u001b[0m (\u001b[33mllmnerds\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xl0/work/projects/transformers/wandb/run-20230922_221050-1mybai75</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llmnerds/my-awesome-project/runs/1mybai75' target=\"_blank\">absurd-plant-27</a></strong> to <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/1mybai75' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/1mybai75</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3289f99d1d704dadb2cc6f73653333ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.011 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.251615…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▃▇█▇▇██</td></tr><tr><td>loss</td><td>▇█▁▃▂▁▂▁</td></tr><tr><td>val_acc</td><td>▁▃▇█▇▇██</td></tr><tr><td>val_loss</td><td>▇█▁▃▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.82512</td></tr><tr><td>loss</td><td>0.20847</td></tr><tr><td>val_acc</td><td>0.82512</td></tr><tr><td>val_loss</td><td>0.20847</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-plant-27</strong> at: <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/1mybai75' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/1mybai75</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230922_221050-1mybai75/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"my-awesome-project\",\n",
    "#     entity=\"llmnerds\",\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"learning_rate\": 0.02,\n",
    "#         \"architecture\": \"CNN\",\n",
    "#         \"dataset\": \"CIFAR-100\",\n",
    "#         \"epochs\": 10,\n",
    "#         \"sdfsfs\": 12,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # simulate training\n",
    "# epochs = 10\n",
    "# offset = random.random() / 5\n",
    "\n",
    "\n",
    "# for epoch in range(2, epochs):\n",
    "#     acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
    "#     loss = 2**-epoch + random.random() / epoch + offset\n",
    "\n",
    "#     # log metrics to wandb\n",
    "#     wandb.log({\"acc\": acc, \"loss\": loss}, step=epoch)\n",
    "#     wandb.log({\"val_acc\": acc, \"val_loss\": loss}, step=epoch)\n",
    "\n",
    "# # [optional] finish the wandb run, necessary in notebooks\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from lovely_tensors.patch import monkey_patch; monkey_patch()\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def new_gelu(input):\n",
    "    return (\n",
    "        0.5\n",
    "        * input\n",
    "        * (\n",
    "            1.0\n",
    "            + torch.tanh(\n",
    "                math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1d(x, weight, bias=None):\n",
    "    size_out = x.size()[:-1] + (weight.size(-1),)\n",
    "    x = torch.addmm(bias, x.view(-1, x.size(-1)), weight)\n",
    "    x = x.view(size_out)\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_block(i, input_hidden_state, model_state):\n",
    "    def block_state(key):\n",
    "        return model_state[f\"h.{i}.{key}\"]\n",
    "\n",
    "    def attention_state(key):\n",
    "        return model_state[f\"h.{i}.attn.{key}\"]\n",
    "\n",
    "    # attention block\n",
    "    ln1 = torch.nn.functional.layer_norm(\n",
    "        input=input_hidden_state,\n",
    "        weight=block_state(\"ln_1.weight\"),\n",
    "        bias=block_state(\"ln_1.bias\"),\n",
    "        normalized_shape=(768,),\n",
    "    )\n",
    "\n",
    "    w_q, w_k, w_v = attention_state(\"c_attn.weight\").chunk(3, dim=1)\n",
    "    b_q, b_k, b_v = attention_state(\"c_attn.bias\").chunk(3, dim=0)\n",
    "\n",
    "    q = conv_1d(ln1, w_q, b_q)\n",
    "    k = conv_1d(ln1, w_k, b_k)\n",
    "    v = conv_1d(ln1, w_v, b_v)\n",
    "\n",
    "    q_chunked = torch.stack(q.chunk(12, dim=-1))\n",
    "    k_chunked = torch.stack(k.chunk(12, dim=-1))\n",
    "    v_chunked = torch.stack(v.chunk(12, dim=-1))\n",
    "\n",
    "    attention = torch.matmul(q_chunked, k_chunked.transpose(-1, -2))\n",
    "\n",
    "    attention_rescaled = attention / (64**0.5)\n",
    "\n",
    "    mask = torch.triu(torch.ones_like(attention_rescaled), diagonal=1).bool()\n",
    "    attention_masked = attention_rescaled.masked_fill(\n",
    "        mask, torch.finfo(torch.float32).min\n",
    "    )\n",
    "\n",
    "    attention_softmaxed = torch.nn.functional.softmax(attention_masked, dim=-1)\n",
    "    attention_output = torch.matmul(attention_softmaxed, v_chunked)\n",
    "\n",
    "    out_tuple = [x[0] for x in attention_output.chunk(12, dim=0)]\n",
    "    combined_attention_output = torch.cat(out_tuple, dim=-1)\n",
    "\n",
    "    w_cproj = attention_state(\"c_proj.weight\")\n",
    "    b_cproj = attention_state(\"c_proj.bias\")\n",
    "\n",
    "    crosstalk = conv_1d(combined_attention_output, w_cproj, b_cproj)\n",
    "    after_residual = crosstalk + input_hidden_state\n",
    "\n",
    "    # mlp block\n",
    "    before_ln2 = after_residual\n",
    "\n",
    "    ln2 = torch.nn.functional.layer_norm(\n",
    "        input=after_residual,\n",
    "        weight=block_state(\"ln_2.weight\"),\n",
    "        bias=block_state(\"ln_2.bias\"),\n",
    "        normalized_shape=(768,),\n",
    "    )\n",
    "\n",
    "    w_fc = block_state(\"mlp.c_fc.weight\")\n",
    "    b_fc = block_state(\"mlp.c_fc.bias\")\n",
    "\n",
    "    after_up = conv_1d(ln2, w_fc, b_fc)\n",
    "    activated = new_gelu(after_up)\n",
    "\n",
    "    w_proj = block_state(\"mlp.c_proj.weight\")\n",
    "    b_proj = block_state(\"mlp.c_proj.bias\")\n",
    "\n",
    "    after_down = conv_1d(activated, w_proj, b_proj)\n",
    "\n",
    "    after_residual_2 = after_down + before_ln2\n",
    "\n",
    "    return after_residual_2\n",
    "\n",
    "\n",
    "def transformer(token_ids, model_state):\n",
    "    token_embeddings = model_state[\"wte.weight\"][token_ids]\n",
    "    positions = torch.arange(len(token_ids))  # [0,1,2,3...]\n",
    "    position_embeddings = model_state[\"wpe.weight\"][positions]\n",
    "    embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "    hs = embeddings\n",
    "    for i in range(12):\n",
    "        hs = transformer_block(i, hs, model_state)\n",
    "\n",
    "    ln_w = model_state[\"ln_f.weight\"]\n",
    "    ln_b = model_state[\"ln_f.bias\"]\n",
    "\n",
    "    ln = torch.nn.functional.layer_norm(\n",
    "        input=hs, weight=ln_w, bias=ln_b, normalized_shape=(768,)\n",
    "    )\n",
    "\n",
    "    return ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(token_ids, model_state):\n",
    "    transformer_output = transformer(token_ids, model_state)\n",
    "    logits = torch.matmul(transformer_output, model_state[\"wte.weight\"].T)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "model_params = torch.load(\"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 50257] n=502570 (1.9Mb) x∈[-110.039, -29.114] μ=-76.757 σ=15.804"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = language_model(token_ids, model_params)\n",
    "out\n",
    "\n",
    "# out[0].chans(scale=4, cmap=\"seismic\")\n",
    "# out[1].chans(scale=4, cmap=\"seismic\")\n",
    "# out[2].chans(scale=4, cmap=\"seismic\")\n",
    "# out[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tiny_shakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "\n",
    "\n",
    "def decode(l):\n",
    "    return \"\".join(\n",
    "        [itos[i] for i in l]\n",
    "    )  # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "encoded_data = encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_data[: int(len(encoded_data) * 0.8)]\n",
    "val_data = encoded_data[int(len(encoded_data) * 0.8) :]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_data = torch.tensor(train_data).to(device)\n",
    "val_data = torch.tensor(val_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5576.96875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "892315 / (5 * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor[2, 5] i64 n=10 x∈[1, 58] μ=40.400 σ=20.998 cuda:0 [[18, 47, 56, 57, 58], [1, 15, 47, 58, 47]], tensor[2, 5] i64 n=10 x∈[1, 64] μ=45.000 σ=20.580 cuda:0 [[47, 56, 57, 58, 1], [15, 47, 58, 47, 64]])\n",
      "(tensor[2, 5] i64 n=10 x∈[0, 64] μ=37.900 σ=21.886 cuda:0 [[64, 43, 52, 10, 0], [14, 43, 44, 53, 56]], tensor[2, 5] i64 n=10 x∈[0, 56] μ=35.800 σ=20.032 cuda:0 [[43, 52, 10, 0, 14], [43, 44, 53, 56, 43]])\n",
      "(tensor[2, 5] i64 n=10 x∈[1, 61] μ=39.600 σ=21.423 cuda:0 [[43, 1, 61, 43, 1], [54, 56, 53, 41, 43]], tensor[2, 5] i64 n=10 x∈[1, 61] μ=39.600 σ=21.423 cuda:0 [[1, 61, 43, 1, 54], [56, 53, 41, 43, 43]])\n",
      "(tensor[2, 5] i64 n=10 x∈[1, 63] μ=40.000 σ=22.015 cuda:0 [[43, 42, 1, 39, 52], [63, 1, 44, 59, 56]], tensor[2, 5] i64 n=10 x∈[1, 63] μ=41.500 σ=22.741 cuda:0 [[42, 1, 39, 52, 63], [1, 44, 59, 56, 58]])\n",
      "(tensor[2, 5] i64 n=10 x∈[1, 58] μ=39.400 σ=20.001 cuda:0 [[58, 46, 43, 56, 6], [1, 46, 43, 39, 56]], tensor[2, 5] i64 n=10 x∈[1, 56] μ=33.700 σ=22.121 cuda:0 [[46, 43, 56, 6, 1], [46, 43, 39, 56, 1]])\n",
      "(tensor[2, 5] i64 n=10 x∈[1, 57] μ=34.600 σ=22.312 cuda:0 [[1, 51, 43, 1, 57], [54, 43, 39, 49, 8]], tensor[2, 5] i64 n=10 x∈[0, 57] μ=34.500 σ=22.481 cuda:0 [[51, 43, 1, 57, 54], [43, 39, 49, 8, 0]])\n",
      "(tensor[2, 5] i64 n=10 x∈[0, 54] μ=25.100 σ=22.840 cuda:0 [[0, 0, 13, 50, 50], [10, 0, 31, 54, 43]], tensor[2, 5] i64 n=10 x∈[0, 54] μ=29.000 σ=21.359 cuda:0 [[0, 13, 50, 50, 10], [0, 31, 54, 43, 39]])\n",
      "(tensor[2, 5] i64 n=10 x∈[1, 57] μ=34.500 σ=21.230 cuda:0 [[39, 49, 6, 1, 57], [54, 43, 39, 49, 8]], tensor[2, 5] i64 n=10 x∈[0, 57] μ=30.600 σ=23.745 cuda:0 [[49, 6, 1, 57, 54], [43, 39, 49, 8, 0]])\n",
      "(tensor[2, 5] i64 n=10 x∈[0, 58] μ=29.900 σ=25.335 cuda:0 [[0, 0, 18, 47, 56], [57, 58, 1, 15, 47]], tensor[2, 5] i64 n=10 x∈[0, 58] μ=35.700 σ=24.350 cuda:0 [[0, 18, 47, 56, 57], [58, 1, 15, 47, 58]])\n"
     ]
    }
   ],
   "source": [
    "ctx = 5\n",
    "\n",
    "\n",
    "def get_item(data, ctx):\n",
    "    # i = random.randint(0, len(data) - ctx - 1)\n",
    "    i = 0\n",
    "    while i + ctx < len(data):\n",
    "        src = data[i : i + ctx]\n",
    "        dst = data[i + 1 : i + ctx + 1]\n",
    "        yield src, dst\n",
    "        i += ctx\n",
    "\n",
    "\n",
    "def get_batch(data, ctx_len, batch_size):\n",
    "    \"\"\"Yields a tuple of tensors of shape (batch_size, ctx).\n",
    "    X, shape=B C\n",
    "    y, shape=B C\n",
    "    \"\"\"\n",
    "    gen = get_item(data, ctx_len)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            X, y = zip(*[next(gen) for _ in range(batch_size)])\n",
    "            yield torch.stack(X), torch.stack(y)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "\n",
    "for batch in get_batch(train_data[:100], ctx_len=5, batch_size=2):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor cuda:0 4.218"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "CTX_LEN = 1\n",
    "\n",
    "\n",
    "# in_ # BATCH_SIZE x CTX_LEN x len(chars)  (feed in a batch of CTX_LEN embeddings (each is a one-hot-encoded character)\n",
    "# out_ # BATCH_SIZE x len(chars)  (get out a (one-hot-encoded) next-char prediction for each batch-item\n",
    "l1 = torch.nn.Linear(len(chars) * CTX_LEN, len(chars)).to(device)\n",
    "\n",
    "model_params = {\n",
    "    \"w\": l1.weight,\n",
    "    \"b\": l1.bias,\n",
    "}\n",
    "\n",
    "\n",
    "def model(params, input_ids, vocab_size):\n",
    "    \"\"\"This model takes in a sequence and predicts 1 token\"\"\"\n",
    "\n",
    "    one_hot_inputs = torch.nn.functional.one_hot(input_ids, num_classes=vocab_size)\n",
    "    one_hot_inputs = one_hot_inputs.float()\n",
    "\n",
    "    combined_ont_hot_inputs = one_hot_inputs.reshape(\n",
    "        (input_ids.shape[0], -1)\n",
    "    )  # BATCH_SIZE x (CTX_LEN * len(chars))\n",
    "\n",
    "    preds = combined_ont_hot_inputs @ params[\"w\"].transpose(-2, -1) + params[\"b\"]\n",
    "\n",
    "    # preds = torch.rand(\n",
    "    #     (input_ids.shape[0], vocab_size), dtype=torch.float32, device=input_ids.device\n",
    "    # )\n",
    "\n",
    "    return preds\n",
    "\n",
    "batch_gen = get_batch(train_data, ctx_len=CTX_LEN, batch_size=32)\n",
    "X, y = next(batch_gen)\n",
    "with torch.no_grad():\n",
    "    preds = model(model_params, X, vocab_size=len(chars))\n",
    "    loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:v470z0ef) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558d0b33182a44a889661f0b706fbb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.011 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.253356…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>2.70234</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-fire-29</strong> at: <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/v470z0ef' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/v470z0ef</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230922_221600-v470z0ef/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:v470z0ef). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6ae88cd0074956838ba34c756b0676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112420388963073, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xl0/work/projects/transformers/wandb/run-20230922_221624-ld2uktka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llmnerds/my-awesome-project/runs/ld2uktka' target=\"_blank\">apricot-field-30</a></strong> to <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llmnerds/my-awesome-project' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llmnerds/my-awesome-project/runs/ld2uktka' target=\"_blank\">https://wandb.ai/llmnerds/my-awesome-project/runs/ld2uktka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a8720dd1434d418fa0ed0cf42cfa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964d95f92467420b90ba60048c6c48f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b0a2f525524c3f807ac0e8deb65b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6a8f0fa90f480bb81d84e3804106ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a7cc1819634af5a48f962c9c3c52db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613a8e1e1d864271bd068f1625acfcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fb5bd8ae2744419d4978325c68c8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NEPOCH = 10\n",
    "\n",
    "LOG_INTERVAL = 500\n",
    "VALIDATION_INTERVAL = 1_250\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    entity=\"llmnerds\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"ctx\": CTX_LEN,\n",
    "    },\n",
    ")\n",
    "\n",
    "l1 = torch.nn.Linear(len(chars) * CTX_LEN, len(chars)).to(device)\n",
    "model_params = {\n",
    "    \"w\": l1.weight,\n",
    "    \"b\": l1.bias,\n",
    "}\n",
    "\n",
    "i = 1\n",
    "total_loss = 0\n",
    "val_total_loss = 0\n",
    "for epoch in range(NEPOCH):\n",
    "    for X, y in tqdm(get_batch(train_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE)):\n",
    "        preds = model(params=model_params, input_ids=X, vocab_size=len(chars))\n",
    "        loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param in model_params.values():\n",
    "                param -= 1 * param.grad\n",
    "                param.grad.zero_()\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:\n",
    "            wandb.log({\"loss\": total_loss / LOG_INTERVAL}, step=i)\n",
    "            total_loss = 0\n",
    "\n",
    "        if i % VALIDATION_INTERVAL == 0:\n",
    "            j = 0\n",
    "            for X, y in get_batch(val_data, ctx_len=CTX_LEN, batch_size=BATCH_SIZE):\n",
    "                preds = model(params=model_params, input_ids=X, vocab_size=len(chars))\n",
    "                loss = torch.nn.functional.cross_entropy(preds, y[:, -1])\n",
    "                val_total_loss += loss.item()\n",
    "                j += 1\n",
    "            wandb.log({\"val_loss\": val_total_loss / j}, step=i)\n",
    "            val_total_loss = 0\n",
    "        i += 1\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  # abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y  # d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import tensor\n",
    "\n",
    "# y = tensor([range(65)])  # y\n",
    "# y_hat = tensor.rand  # preds\n",
    "\n",
    "# ce_loss = -torch.log(y_hat[y == 1]).mean()\n",
    "# ce_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "y = torch.tensor([3])  # Class index\n",
    "# y_hat = torch.tensor([[0.1, 0.1, 0.1, 0.5, 0.1, 0.1]])  # Logits\n",
    "y_hat = torch.rand((1, 65)).float()  # Logits\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(y_hat, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([1, 2, 3, 4, 5])\n",
    "A == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([10, 20, 30, 40, 50])\n",
    "B[A == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
